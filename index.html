<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Quande Liu, Quande Liu, CSE, CUHK, The Chinese University of Hong Kong, Zhejiang University"> 
<meta name="description" content="Quande Liu&#39;s home page">
<meta name="google-site-verification" content="X2QFrl-bPeg9AdlMt4VKT9v6MJUSTCf-SrY3CvKt4Zs" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Quande Liu&#39;s Homepage</title>
<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
</head>
<body>
<div id="layout-content" style="margin-top:25px">
 <a href="https://github.com/liuquande" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#FD6C6C; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

<table>
	<tbody>
		<tr>
			<td width="500">
				<div id="toptitle">					
					<h1>Quande Liu</h1><h1>
				</h1></div>

				<h5>Visual Generation Group (Kling Team)</h5>
				<h5>Kuaishou Technology</h5>
				<h5>Shenzhen, Guangdong, China</h5>

				<p>
				   
					<em>Email: <a href="mailto:qdliu0226@gmail.com">qdliu0226@gmail.com</a></em> <br>
				</p>
				<p> <!--<a href="https://scholar.google.com/citations?user=bRe3FlcAAAAJ&hl=en"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>--> 
					<a href="https://scholar.google.com.hk/citations?hl=en&user=akLlfUoAAAAJ"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/liuquande"><img src="./pic/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://www.researchgate.net/profile/Quande_Liu"><img src="./pic/rg.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://www.linkedin.com/in/quande-liu-b85b56193/"><img src="./pic/LinkedIn_s.png" height="30px" style="margin-bottom:-3px"></a>
				</p>
			</td>
			<td>
				<img src="./pic/qdliu6.jpeg" border="0" width="360"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<h2>Biography </h2>
<p>
	I am currently a Senior Researcher at Kling Team, Kuaishou Technology, focuing on multi-model visual generation and world modeling.</a></b> Previously, I was a senior researcher at <a href="https://www.lightspeed-studios.com/">Tencent LightSpeed Studios</a>.    
    I got my Ph.D degree from the <a href="http://www.cse.cuhk.edu.hk/">Department of Computer Science and Engineering</a>,  
	<a href="http://www.cuhk.edu.hk/">The Chinese University of Hong Kong</a>, and received the B. Eng (honors) degree from <a href="http://ckc.zju.edu.cn/">Chu Kochen Honors College</a>, <a href="http://www.zju.edu.cn/">Zhejiang University</a> in 2018</a>.
</p>

<p>Recently, I mainly study <u>multi-model visual generation</u> and <u>generative world modeling</u>. </p>
<p style="color: red; display: inline;">We are actively looking for interns to work on cutting-edge research topics in video generation and multi-modal learning. Feel free to email me if you are interested. </p>

</p>


<h2>News</h2>
<ul>
	<li>
		[12/2025] We release <a href="https://arxiv.org/abs/2512.16776">Kling-O1</a>, welcome to check it out.
	</li>
	<li>
		[10/2025] Awarded Stanford World's Top 2% Scientists 2025</a>.
	</li>
	<li>
		[08/2025] Our work <a href="https://context-as-memory.github.io/">Context as Memory</a> for generative world modeling with video-gen is accepted to SIGGRAPH Asia 2025.
	</li>
	<li>
		[07/2025] We release <a href="https://fulldit2.github.io/">FullDiT2</a> and <a href="https://unicedit.github.io/">UNIC</a>, for efficient multi-modal video geneartion and editing.
	</li>
	<li>
		[06/2025] Our work <a href="https://fulldit.github.io/">FullDiT</a> is accepted to ICCV 2025.
	</li>
	<li>
		[04/2025] We release <a href="https://arxiv.org/abs/2503.17359">IGV-GGE</a>, and <a href="https://arxiv.org/abs/2503.24379">Any2Caption</a> for interactive video generation and multi-modal video understanding.
	</li>
	<li>
		[01/2025] Our work <a href="https://github.com/GameGen-X/GameGen-X">GameGen-X</a> for open-world video game gneration was accepted by ICLR 2025.
	</li>
	<li>
		[10/2024] Awarded <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/7">Top 2% Scientists Worldwide 2024</a> by Stanford University.
	</li>
	<li>
		[04/2024] Our work for zero-shot human video generation (i.e., ID-Animator) was open-sourced.
	</li>
	<li>
		[08/2023] Great honor to be awarded with the <a href="https://www.cse.cuhk.edu.hk/category/news/achievements/">CUHK Young Scholar Thesis Award</a> !
	</li>
	<li>
		[05/2022] I passed my oral defense and become a <p style="color: black; display: inline;">Ph.D.</p>, at the day of May 19, 2022!
	</li>
	<li>
		[07/2022] Paper on vision-language-driven semantic segmentation (ViL-Seg) was accepted by ECCV'22.
	</li>
	<li>
		[12/2021] Paper on single domain generalization for semantic segmentation was accepted by AAAI'22.
	</li>
	<li>
		[09/2021] We won the <p style="color: black; display: inline;">champion</p> of <a href="https://fets-ai.github.io/Challenge/">Federated Tumor Segmentation Challenge (FeTS)</a>!
	</li>
	<li>
		[03/2021] Paper on federated domain generalization (FedDG) was accepted by CVPR'21. 
	</li>
	<li>
		[02/2021] Paper on federated learning for COVID-19 lesion detection was accepted by npj Digital Medicine. 
	</li>
	<li>
		[09/2020] Fortunate to receive <p style="color: black; display: inline;"><a href="https://www.microsoft.com/en-us/research/academic-program/fellowships-microsoft-research-asia/">MSRA PhD Fellowship Nomination Award</a></p>.
	</li>
</ul>



<h2 class="selected-publications"> -  Recent Publications    | -  <a href="https://scholar.google.com.hk/citations?hl=en&user=akLlfUoAAAAJ">Full List</a></h2>
<p class="contribution-note">(* indicates equal contributions, # indicates corresponding author)</p>

<table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<img src="./indexpics/O1.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td>		
		<td> <b>Kling-Omni Technical Report</b><br>
		Kling-Team <br>
		<em>ArXiv Preprint, 2025.
		<p>[<a href="https://arxiv.org/abs/2512.16776">paper</a>][<a href="https://app.klingai.com/global/omni/new">Interface</a>]
		</td>
	</tr>

<table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<img src="./indexpics/siga25_cam.jpg" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td>		
		<td> <b>Context as Memory</b>: Scene-Consistent Interactive Long Video Generation with Memory Retrieval <br>
		Jiwen Yu, Jianhong Bai, Yiran Qin, <b>Quande Liu<sup>#</sup></b>, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu<sup>#</sup> <br>
		<em>SIGGRAPH Asia 2025.
		<p>[<a href="https://arxiv.org/pdf/2506.03141">paper</a>][<a href="https://context-as-memory.github.io/">project page</a>]
		</td>
	</tr>

<table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<img src="./indexpics/fulldit2.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td>		
		<td> <b>FullDiT2</b>: Efficient In-Context Conditioning for Video Diffusion Transformers. <br>
		Xuanhua He, <b>Quande Liu<sup>#</sup></b>, Zixuan Ye, Wecai Ye, Qiulin Wang, Xintao Wang, Qifeng Chen, Pengfei Wan, Di Zhang, Kun Gai <br>
		<em>ArXiv Preprint, 2025.
		<p>[<a href="https://arxiv.org/pdf/2506.04213">paper</a>][<a href="https://fulldit2.github.io/">project page</a>]
		</td>
	</tr>



<table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<img src="./indexpics/unic25.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td>		
		<td> <b>UNIC</b>: Unified In-Context Video Editing. <br>
		Zixuan Ye*, Xuanhua He*, <b>Quande Liu<sup>#</sup></b>, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, Wenhan Luo <br>
		<em>ArXiv Preprint, 2025.
		<p>[<a href="https://arxiv.org/abs/2506.04216">paper</a>][<a href="https://unicedit.github.io/">project page</a>]
		</td>
	</tr>


<table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<img src="./indexpics/igvsurvey25.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td>		
		<td> <b>A Survey of Interactive Generative Video.</b> <br>
		Jiwen Yu, Yiran Qin, Haoxuan Che, <b>Quande Liu<sup>#</sup></b>, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Hao Chen, Xihui Liu <br>
		<em>ArXiv Preprint, 2025.
		<p>[<a href="https://arxiv.org/abs/2504.21853">paper</a>]
		</td>
	</tr>
	
    


<table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<img src="./indexpics/fulldit2025.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td>		
		<td> <b>FullDiT</b>: Multi-Task Video Generative Foundation Model with Full Attention. <br>
		Xuan Ju, Weicai Ye, <b>Quande Liu</b>, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qiang Xu <br>
		<em>International Conference on Computer Vision (ICCV), 2025.
		<p>[<a href="https://arxiv.org/abs/2503.19907">paper</a>][<a href="https://fulldit.github.io/">project page</a>]
		</td>
	</tr>
	
    

<table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<img src="./indexpics/position2025.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td>		
		<td> <b>Position</b>: Interactive Generative Video as Next-Generation Game Engine. <br>
		Jiwen Yu, Yiran Qin, Haoxuan Che, <b>Quande Liu</b>, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu <br>
		<em>ArXiv Preprint, 2025.
		<p>[<a href="https://arxiv.org/abs/2503.17359">paper</a>]
		</td>
	</tr>
	
    <table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<video muted autoplay="autoplay" loop="loop" width="250px">
			<source src="./indexpics/OGameGen_Func_compressed.mp4" type="video/mp4"></source>
		</video>
		</td>		
		<td> <b>GameGen-X</b>: Interactive Open-World Game Video Generation.<br>
		Haoxuan Che*, Xuanhua He*, <b>Quande Liu<sup>#</sup></b>, Cheng Jin, Hao Chen <br>
		<em>International Conference on Learning Representations (ICLR), 2025.
		<p>[<a href="https://arxiv.org/abs/2411.00769">paper</a>][<a href="https://github.com/GameGen-X/GameGen-X">code</a>][<a href="https://gamegen-x.github.io/">project page</a>]
		</td>
	</tr>
	


    

<table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<video muted autoplay="autoplay" loop="loop" width="250px">
			<source src="./indexpics/arxiv_textanimator.mp4" type="video/mp4"></source>
		</video>
		</td>		
		<td> <b>Text-Animator</b>: Controllable Visual Text Video Generation. <br>
		Lin Liu, <b>Quande Liu</b>, Shengju Qian, Yuan Zhou, Wengang Zhou, Houqiang Li, Lingxi Xie, Qi Tian <br>
		<em>ArXiv Preprint, 2024.
		<p>[<a href="https://laulampaul.github.io/text-animator.html">paper</a>][code][<a href="https://laulampaul.github.io/text-animator.html">project page</a>]
		</td>
	</tr>
	
    

<table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<video muted autoplay="autoplay" loop="loop" width="250px">
			<source src="./indexpics/arxiv_idanimator.mp4" type="video/mp4"></source>
		</video>
		</td>		
		<td> <b>ID-Animator</b>: Zero-Shot Identity-Preserving Human Video Generation. <br>
		Xuanhua He, <b>Quande Liu<sup>#</sup></b>, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, Man Zhou, Jie Zhangâ€ . <br>
		<em>ArXiv Preprint, 2024.
		<p>[<a href="https://arxiv.org/abs/2404.15275">paper</a>][<a href="https://github.com/ID-Animator/ID-Animator">code</a>][<a href="https://id-animator.github.io/">project page</a>]
		</td>
	</tr>
	
    

<table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<img src="./indexpics/eccv22_ViL-Seg.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td>		
		<td> Open-world Semantic Segmentation via Contrasting and Clustering Vision-Language Embedding. <br>
		<b>Quande Liu</b>, Youpeng Wen, Jianhua Han, Chunjing Xu, Hang Xu, Xiaodan Liang. <br>
		<em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2022.
		<p>[<a href="https://arxiv.org/pdf/2207.08455.pdf">paper</a>]
		</td>
	</tr>
	
    

<table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<img src="./indexpics/aaai22_sdg.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td>		
		<td> Single-domain Generalization in Image Segmentation via Test-time Adaptation from Shape Dictionary. <br>
		<b>Quande Liu</b>, Cheng Chen, Qi Dou, Pheng Ann Heng. <br>
		<em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>)</em>, 2022.
		<p>[<a href="https://www.aaai.org/AAAI22Papers/AAAI-852.LiuQ.pdf">paper</a>]
		</td>
	</tr>
	
    

<table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<img src="./indexpics/cvpr21_feddg.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center"> 
		</td>		
		<td> <b>FedDG</b>: Federated Domain Generalization on Image Segmentation via Episodic Learning in Continuous Frequency Space. <br>
		<b>Quande Liu</b>, Cheng Chen, Jing Qin, Qi Dou, Pheng-Ann Heng <br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021.
		<p>[<a href="https://arxiv.org/pdf/2103.06030.pdf">paper</a>][<a href="https://github.com/liuquande/FedDG-ELCFS">code</a>]
		</td>
	</tr>
	
    

<table id="tbPublications" width="100%">
	<tbody>
		<td width="270">
		<img src="./indexpics/npj21_covid.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
		</td>		
		<td> Federated Deep Learning for Detecting COVID-19 Lung Abnormalities in CT: A Privacy-preserving Multinational Validation Study. <br>
		Qi Dou, Tiffany Y So, Meirui Jiang, <b>Quande Liu</b>, Varut Vardhanabhuti, Georgios Kaissis, Zeju Li, Weixin Si, Heather Lee, Kevin Yu, Zuxin Feng, Li Dong, Egon Burian, Friederike Jungmann, Rickmer Braren, Marcus Makowski, Bernhard Kainz, Daniel Rueckert, Ben Glocker, Simon Yu, Pheng Ann Heng<br>
		<em><b>npj Digital Medicine</b>, Nature Publishing Group</em>, 2021.
		<p>[<a href="https://rdcu.be/chFTy">paper</a>][<a href="https://github.com/med-air/FL-COVID">code</a>]
		</td>
	</tr>
	
    

</tbody></table>


<h2>Honors &amp; Awards</h2>
<ul>
	<li>
		<tr><td> Ranked as <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/7">Top 2% Scientists Worldwide 2024 (Single Year)</a> by Stanford University, 2024 </td></tr>
	</li>
	<li>
		<tr><td> CUHK Young Scholar Thesis Award, 2023 </td></tr>
	</li>
	<li>
		<tr><td> Itarle Scholarship, 2022 </td></tr>
	</li>
	<li>
		<tr><td> <a href="https://www.msra.cn/zh-cn/connections/academic-programs/fellows">MSRA PhD Fellowship Nomination Award</a>, Microsoft Inc., 2020 </td></tr>
	</li>
	<!-- <a href="https://www.msra.cn/zh-cn/connections/academic-programs/fellows">Nominee of MSRA fellowship</a> (<b>27 nominees</b> from Asia) -->
	<li>
		<tr><td> Outstanding Graduates Award, Zhejiang Province, 2018</td></tr>
	</li>
	<li>
		<tr><td> Zhejiang University Scholarship, 2015-2017</td></tr>
	</li>
	<li>
		<tr><td> Admission to Chu Kochen Honors College, Zhejiang University, 2014</td></tr>
	</li>
</ul>


<h2>Professional Activities</h2>
<ul>
	<li>	
	<b>Conference Reviews:</b><br>
	IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2021<br>
	AAAI Conference on Artificial Intelligence (AAAI) 2021<br>
	International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) 2020 <br>
	</li>
	<li>	
	<b>Journal Reviews:</b><br>
	IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) <br>
	IEEE Transactions on Medical Imaging (TMI) <br>
	Medical Image Analysis (MedIA) <br>
	IEEE Transactions on Image Processing (TIP) <br>
	IEEE Journal of Biomedical and Health Informatics (JBHI)<br>
	IEEE Access <br>
	Neurocomputing <br>
	Computer Methods and Programs in Biomedicine (CMPB) <br>
	<p style="margin-top:3px"></p>		
	</li>
</ul>


<h2>Teaching</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2019-2020</td><td>Spring</td><td>Principles of Programming Languages (CSCI 3180)</td>
		</tr>
		<tr>
			<td> 2019-2020</td><td>Fall</td><td>Problem Solving by Programming (ENGG 1110)</td>
		</tr>
		<tr>
			<td> 2018-2019</td><td>Spring</td><td>Problem Solving by Programming (ENGG 1110)</td>
		</tr>
		<tr>
			<td> 2018-2019</td><td>Fall</td><td>Digital Logic and Systems (ENGG 2020).</td>
		</tr>
	</tbody>
</table>

<div id="footer">
	<div id="footer-text"></div>
</div>
	<p><center>
      	<div id="clustrmaps-widget" style="width:40%">
	<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=360&t=tt&d=LJNWkxUAFjdgZdHhjWvEOF1K9cIg45om0jzghCyXpkc&co=2d78ad&cmo=ff0000&cmn=ffb800&ct=ffffff'></script>
<!-- 		<noscript><a href='https://clustrmaps.com/site/xfn5'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=pT8r_ZMBBdBPTv7KnlTCiBDylmHyi1qsWdPpY_tIlqY'/></a></noscript> -->
	</div>  
	
	<br>
        &copy; Quande Liu | Last updated: Mar 2021
     
      </center></p>


</div>

</body></html>
