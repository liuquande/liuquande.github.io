<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Quande Liu, Quande Liu, CSE, CUHK, The Chinese University of Hong Kong, Zhejiang University"> 
<meta name="description" content="Quande Liu&#39;s home page">
<meta name="google-site-verification" content="X2QFrl-bPeg9AdlMt4VKT9v6MJUSTCf-SrY3CvKt4Zs" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Quande Liu&#39;s Homepage</title>
<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
</head>
<body>
<div id="layout-content" style="margin-top:25px">
 <a href="https://github.com/liuquande" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#FD6C6C; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

<div id="biography-container" style="max-width: 800px; margin: 0 auto;">
    <table>
        <tbody>
            <tr>
                <td width="500">
                    <div id="toptitle">                 
                        <h1>Quande Liu</h1>
                    </div>
                    <p>
                        <h5>Kling Team, Kuaishou Technology</h5>
                    </p>
                    <p>
                    </p>
                    <p>
                        <a href="#" onclick="alert('qdliu0226@gmail.com'); return false;">Email</a> /
                        <a href="https://scholar.google.com.hk/citations?hl=en&user=akLlfUoAAAAJ">Google Scholar</a> /
                        <a href="https://dblp.org/pid/226/6174.html">DBLP</a>
                    </p>
                </td>
                <td>
                    <img src="./pic/qdliu6.jpeg" border="0" width="360"><br>
                </td>
            </tr>
        </tbody>
    </table>

    <h2>Biography</h2>
    <p>
        I am a senior researcher at Kling Team, Kuaishou Technology. We focus on multi-model visual generation and world models. Previously, I was a senior researcher at Tencent. I got my Ph.D degree from The Chinese University of Hong Kong, and B. Eng degree from Chu Kochen Honors College, Zhejiang University.
    </p>
    <p>
        We are actively looking for interns and researchers. Feel free to reach me out if you are interested.
    </p>
</div>

<div id="content-container" style="max-width: 800px; margin: 0 auto;">
    <h2>Recent News</h2>
    <ul>
        <li>
            [02/2026] Two papers are accepted to CVPR 2026.
        </li>
        <li>
            [01/2026] Two papers of UNIC and UniVideo for unified video generation / editing and understanding are accepted to ICLR 2026.
        </li>
        <li>
            [12/2025] We release <a href="https://arxiv.org/abs/2512.16776">Kling-O1</a>.
        </li>
        <li>
            [10/2025] Awarded Stanford World's Top 2% Scientists 2025</a>.
        </li>
        <li>
            [08/2025] <a href="https://context-as-memory.github.io/">Context as Memory</a> for generative world modeling was accepted by SIGGRAPH Asia 2025.
        </li>
        <li>
            [07/2025] We release <a href="https://fulldit2.github.io/">FullDiT2</a> and <a href="https://unicedit.github.io/">UNIC</a> for efficient multimodal video generation and editing.
        </li>
        <li>
            [06/2025] Our work <a href="https://fulldit.github.io/">FullDiT</a> is accepted to ICCV 2025.
        </li>
        <li>
            [04/2025] We release <a href="https://arxiv.org/abs/2503.17359">IGV-GGE</a> for interactive video generation.
        </li>
        <li>
            [01/2025] Our work <a href="https://github.com/GameGen-X/GameGen-X">GameGen-X</a> for open-world video game generation was accepted by ICLR 2025.
        </li>
        <li>
            [10/2024] Awarded <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/7">Stanford World's Top 2% Scientists 2024</a>.
        </li>
        <li>
            [08/2023] Awarded with the <a href="https://www.cse.cuhk.edu.hk/category/news/achievements/">CUHK Young Scholar Thesis Award</a>.
        </li>
        
    </ul>

    <h2 class="selected-publications"> -  Recent Publications    | -  <a href="https://scholar.google.com.hk/citations?hl=en&user=akLlfUoAAAAJ">Full List</a></h2>
    <p class="contribution-note">(* indicates equal contributions, # indicates corresponding author)</p>

    <table id="tbPublications" width="100%">
        <tbody>
            <td width="270">
            <img src="./indexpics/O1.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
            </td>       
            <td> <b>Kling-Omni Technical Report</b><br>
            Kling-Team <br>
            <em>ArXiv Preprint, 2025.
            <p>[<a href="https://arxiv.org/abs/2512.16776">paper</a>][<a href="https://app.klingai.com/global/omni/new">Interface</a>]
            </td>
        </tr>

    <table id="tbPublications" width="100%">
        <tbody>
            <td width="270">
            <img src="./indexpics/univideo.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
            </td>       
            <td> <b>UniVideo</b>: Unified Understanding, Generation, and Editing for Videos <br>
            Cong Wei, <b>Quande Liu<sup>#</sup></b>, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhu Chen<sup>#</sup> <br>
            <em>ICLR 2026.
            <p>[<a href="https://arxiv.org/abs/2510.08377">paper</a>][<a href="https://congwei1230.github.io/UniVideo/">project page</a>]
            </td>
        </tr>

    <table id="tbPublications" width="100%">
        <tbody>
            <td width="270">
            <img src="./indexpics/unic25.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
            </td>       
            <td> <b>UNIC</b>: Unified In-Context Video Editing. <br>
            Zixuan Ye*, Xuanhua He*, <b>Quande Liu<sup>#</sup></b>, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, Wenhan Luo <br>
            <em>ICLR 2026.
            <p>[<a href="https://arxiv.org/abs/2506.04216">paper</a>][<a href="https://unicedit.github.io/">project page</a>]
            </td>
        </tr>

    <table id="tbPublications" width="100%">
        <tbody>
            <td width="270">
            <img src="./indexpics/siga25_cam.jpg" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
            </td>       
            <td> <b>Context as Memory</b>: Scene-Consistent Interactive Long Video Generation with Memory Retrieval <br>
            Jiwen Yu, Jianhong Bai, Yiran Qin, <b>Quande Liu<sup>#</sup></b>, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu<sup>#</sup> <br>
            <em>SIGGRAPH Asia 2025.
            <p>[<a href="https://arxiv.org/pdf/2506.03141">paper</a>][<a href="https://context-as-memory.github.io/">project page</a>]
            </td>
        </tr>

    <table id="tbPublications" width="100%">
        <tbody>
            <td width="270">
            <img src="./indexpics/fulldit2.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
            </td>       
            <td> <b>FullDiT2</b>: Efficient In-Context Conditioning for Video Diffusion Transformers. <br>
            Xuanhua He, <b>Quande Liu<sup>#</sup></b>, Zixuan Ye, Wecai Ye, Qiulin Wang, Xintao Wang, Qifeng Chen, Pengfei Wan, Di Zhang, Kun Gai <br>
            <em>ArXiv Preprint, 2025.
            <p>[<a href="https://arxiv.org/pdf/2506.04213">paper</a>][<a href="https://fulldit2.github.io/">project page</a>]
            </td>
        </tr>



    <table id="tbPublications" width="100%">
        <tbody>
            <td width="270">
            <img src="./indexpics/igvsurvey25.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
            </td>       
            <td> <b>A Survey of Interactive Generative Video.</b> <br>
            Jiwen Yu, Yiran Qin, Haoxuan Che, <b>Quande Liu<sup>#</sup></b>, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Hao Chen, Xihui Liu <br>
            <em>ArXiv Preprint, 2025.
            <p>[<a href="https://arxiv.org/abs/2504.21853">paper</a>]
            </td>
        </tr>

    <table id="tbPublications" width="100%">
        <tbody>
            <td width="270">
            <img src="./indexpics/fulldit2025.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
            </td>       
            <td> <b>FullDiT</b>: Multi-Task Video Generative Foundation Model with Full Attention. <br>
            Xuan Ju, Weicai Ye, <b>Quande Liu</b>, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qiang Xu <br>
            <em>International Conference on Computer Vision (ICCV), 2025.
            <p>[<a href="https://arxiv.org/abs/2503.19907">paper</a>][<a href="https://fulldit.github.io/">project page</a>]
            </td>
        </tr>

    <table id="tbPublications" width="100%">
        <tbody>
            <td width="270">
            <img src="./indexpics/position2025.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
            </td>       
            <td> <b>Position</b>: Interactive Generative Video as Next-Generation Game Engine. <br>
            Jiwen Yu, Yiran Qin, Haoxuan Che, <b>Quande Liu</b>, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu <br>
            <em>ArXiv Preprint, 2025.
            <p>[<a href="https://arxiv.org/abs/2503.17359">paper</a>]
            </td>
        </tr>

    <table id="tbPublications" width="100%">
        <tbody>
            <td width="270">
            <video muted autoplay="autoplay" loop="loop" width="250px">
                <source src="./indexpics/OGameGen_Func_compressed.mp4" type="video/mp4"></source>
            </video>
            </td>       
            <td> <b>GameGen-X</b>: Interactive Open-World Game Video Generation.<br>
            Haoxuan Che*, Xuanhua He*, <b>Quande Liu<sup>#</sup></b>, Cheng Jin, Hao Chen <br>
            <em>International Conference on Learning Representations (ICLR), 2025.
            <p>[<a href="https://arxiv.org/abs/2411.00769">paper</a>][<a href="https://github.com/GameGen-X/GameGen-X">code</a>][<a href="https://gamegen-x.github.io/">project page</a>]
            </td>
        </tr>

    <table id="tbPublications" width="100%">
        <tbody>
            <td width="270">
            <video muted autoplay="autoplay" loop="loop" width="250px">
                <source src="./indexpics/arxiv_textanimator.mp4" type="video/mp4"></source>
            </video>
            </td>       
            <td> <b>Text-Animator</b>: Controllable Visual Text Video Generation. <br>
            Lin Liu, <b>Quande Liu</b>, Shengju Qian, Yuan Zhou, Wengang Zhou, Houqiang Li, Lingxi Xie, Qi Tian <br>
            <em>ArXiv Preprint, 2024.
            <p>[<a href="https://laulampaul.github.io/text-animator.html">paper</a>][code][<a href="https://laulampaul.github.io/text-animator.html">project page</a>]
            </td>
        </tr>

    <table id="tbPublications" width="100%">
        <tbody>
            <td width="270">
            <video muted autoplay="autoplay" loop="loop" width="250px">
                <source src="./indexpics/arxiv_idanimator.mp4" type="video/mp4"></source>
            </video>
            </td>       
            <td> <b>ID-Animator</b>: Zero-Shot Identity-Preserving Human Video Generation. <br>
            Xuanhua He, <b>Quande Liu<sup>#</sup></b>, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, Man Zhou, Jie Zhangâ€ . <br>
            <em>ArXiv Preprint, 2024.
            <p>[<a href="https://arxiv.org/abs/2404.15275">paper</a>][<a href="https://github.com/ID-Animator/ID-Animator">code</a>][<a href="https://id-animator.github.io/">project page</a>]
            </td>
        </tr>

    <table id="tbPublications" width="100%">
        <tbody>
            <td width="270">
            <img src="./indexpics/eccv22_ViL-Seg.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center">
            </td>       
            <td> Open-world Semantic Segmentation via Contrasting and Clustering Vision-Language Embedding. <br>
            <b>Quande Liu</b>, Youpeng Wen, Jianhua Han, Chunjing Xu, Hang Xu, Xiaodan Liang. <br>
            <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2022.
            <p>[<a href="https://arxiv.org/pdf/2207.08455.pdf">paper</a>]
            </td>
        </tr>


    <table id="tbPublications" width="100%">
        <tbody>
            <td width="270">
            <img src="./indexpics/cvpr21_feddg.png" width="250px" style="box-shadow: 4px 4px 8px #888" class="center"> 
            </td>       
            <td> <b>FedDG</b>: Federated Domain Generalization on Image Segmentation via Episodic Learning in Continuous Frequency Space. <br>
            <b>Quande Liu</b>, Cheng Chen, Jing Qin, Qi Dou, Pheng-Ann Heng <br>
            <em>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2021.
            <p>[<a href="https://arxiv.org/pdf/2103.06030.pdf">paper</a>][<a href="https://github.com/liuquande/FedDG-ELCFS">code</a>]
            </td>
        </tr>

    </table>

    <h2>Honors & Awards</h2>
    <ul>
        <li>
            <tr><td> Ranked as <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/7">Top 2% Scientists Worldwide 2025 (Single Year)</a> by Stanford University, 2025 </td></tr>
        </li>
        <li>
            <tr><td> Ranked as <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/7">Top 2% Scientists Worldwide 2024 (Single Year)</a> by Stanford University, 2024 </td></tr>
        </li>
        <li>
            <tr><td> CUHK Young Scholar Thesis Award, 2023 </td></tr>
        </li>
        <li>
            <tr><td> Itarle Scholarship, 2022 </td></tr>
        </li>
        <li>
            <tr><td> <a href="https://www.msra.cn/zh-cn/connections/academic-programs/fellows">MSRA PhD Fellowship Nomination Award</a>, Microsoft Inc., 2020 </td></tr>
        </li>
    </ul>
    <h2>Professional Activities</h2>
    <ul>
        <li>Conference Reviewer for ICLR, ICML, NeurIPS, CVPR, ICCV, ECCV, AAAI</li>
        <li>Journal Reviewer for TPAMI, TIP, TNNLS</li>
    </ul>
</div>

</body></html>
